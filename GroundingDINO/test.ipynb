{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到的框数量: 4\n",
      "平均置信度 (Average Score): 0.6700\n",
      "最高 Score: 0.8263\n",
      "最低 Score: 0.5957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"/home/pearson/Projects/ECE253/GroundingDINO/weights/groundingdino_swint_ogc.pth\")\n",
    "IMAGE_PATH = \"/home/pearson/Projects/ECE253/Image_process/example_jpg/output_CLAHE/output_clahe_IMG_3580.jpeg\"\n",
    "TEXT_PROMPT = \"person . car .\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"output_clahe_IMG_3580_annoted.jpg\", annotated_frame)\n",
    "\n",
    "num_boxes = len(boxes)\n",
    "\n",
    "if num_boxes > 0:\n",
    "    # 2. 计算平均 Score (logits 是 Tensor，使用 .mean() 计算平均值，.item() 转为 Python float)\n",
    "    avg_score = logits.mean().item()\n",
    "    \n",
    "    print(f\"检测到的框数量: {num_boxes}\")\n",
    "    print(f\"平均置信度 (Average Score): {avg_score:.4f}\")\n",
    "    \n",
    "    # 如果你想看最高和最低分：\n",
    "    print(f\"最高 Score: {logits.max().item():.4f}\")\n",
    "    print(f\"最低 Score: {logits.min().item():.4f}\")\n",
    "else:\n",
    "    print(\"未检测到任何目标。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model from /home/pearson/Projects/ECE253/GroundingDINO/weights/groundingdino_swint_ogc.pth ...\n",
      "final text_encoder_type: bert-base-uncased\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing Folder: ../dataset\n",
      "Output Directory:  ../results/annoted/original/\n",
      "Images Found:      0\n",
      "------------------------------------------------------------\n",
      "No images found. Skipping.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing Folder: ../results/CLAHE/\n",
      "Output Directory:  ../results/annoted/CLAHE/\n",
      "Images Found:      0\n",
      "------------------------------------------------------------\n",
      "No images found. Skipping.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing Folder: ../results/CLAHE_Wiener/\n",
      "Output Directory:  ../results/annoted/CLAHE_Wiener/\n",
      "Images Found:      0\n",
      "------------------------------------------------------------\n",
      "No images found. Skipping.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing Folder: ../results/CLAHE_NAFNet/\n",
      "Output Directory:  ../results/annoted/CLAHE_NAFNet/\n",
      "Images Found:      0\n",
      "------------------------------------------------------------\n",
      "No images found. Skipping.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing Folder: ../results/ZeroDCE/\n",
      "Output Directory:  ../results/annoted/ZeroDCE/\n",
      "Images Found:      0\n",
      "------------------------------------------------------------\n",
      "No images found. Skipping.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing Folder: ../results/ZeroDCE_Wiener/\n",
      "Output Directory:  ../results/annoted/ZeroDCE_Wiener/\n",
      "Images Found:      0\n",
      "------------------------------------------------------------\n",
      "No images found. Skipping.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing Folder: ../results/ZeroDCE_NAFNet/\n",
      "Output Directory:  ../results/annoted/ZeroDCE_NAFNet/\n",
      "Images Found:      0\n",
      "------------------------------------------------------------\n",
      "No images found. Skipping.\n",
      "All batch processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np  \n",
    "from tqdm import tqdm\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "\n",
    "def process_single_folder(model, input_dir, output_dir, config):\n",
    "    \"\"\"\n",
    "    处理单个文件夹，并计算详细统计数据。\n",
    "    \"\"\"\n",
    "    # 解包配置\n",
    "    text_prompt = config[\"text_prompt\"]\n",
    "    box_threshold = config[\"box_threshold\"]\n",
    "    text_threshold = config[\"text_threshold\"]\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 获取图片列表\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.PNG', '*.JPG', '*.JPEG', '*.BMP']\n",
    "    image_paths = []\n",
    "    for ext in image_extensions:\n",
    "        image_paths.extend(glob.glob(os.path.join(input_dir, ext)))\n",
    "    \n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Processing Folder: {input_dir}\")\n",
    "    print(f\"Output Directory:  {output_dir}\")\n",
    "    print(f\"Images Found:      {len(image_paths)}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "\n",
    "    if not image_paths:\n",
    "        print(\"No images found. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # --- 数据容器 ---\n",
    "    # 1. 存储该文件夹下所有图片的所有 Score，用于计算文件夹级别的 Mean 和 Variance\n",
    "    all_scores_in_folder = []\n",
    "    # 2. 存储每张图片的详细信息 (如果你后续需要导出到Excel，可以用这个列表)\n",
    "    per_image_stats = []\n",
    "    # 3. 文件夹总框数计数器\n",
    "    folder_total_boxes = 0\n",
    "\n",
    "    # --- 循环处理图片 ---\n",
    "    for img_path in tqdm(image_paths, desc=\"Progress\"):\n",
    "        filename = os.path.basename(img_path)\n",
    "        \n",
    "        try:\n",
    "            image_source, image = load_image(img_path)\n",
    "\n",
    "            boxes, logits, phrases = predict(\n",
    "                model=model,\n",
    "                image=image,\n",
    "                caption=text_prompt,\n",
    "                box_threshold=box_threshold,\n",
    "                text_threshold=text_threshold,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # 保存可视化结果\n",
    "            annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "            cv2.imwrite(os.path.join(output_dir, f\"annotated_{filename}\"), annotated_frame)\n",
    "\n",
    "            # --- 收集单张图片统计信息 ---\n",
    "            current_count = len(boxes)\n",
    "            # 将 Tensor 转为 list\n",
    "            current_scores = logits.tolist() \n",
    "\n",
    "            # 存储单张图片数据\n",
    "            per_image_stats.append({\n",
    "                \"filename\": filename,\n",
    "                \"box_count\": current_count,\n",
    "                \"scores\": current_scores\n",
    "            })\n",
    "\n",
    "            # 更新文件夹级别的汇总数据\n",
    "            folder_total_boxes += current_count\n",
    "            all_scores_in_folder.extend(current_scores)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    # --- 计算文件夹级别的统计 ---\n",
    "    print(f\"\\n>>> STATS REPORT FOR FOLDER: {os.path.basename(input_dir)}\")\n",
    "    \n",
    "    # 1. 总框数\n",
    "    print(f\"Total Boxes Detected: {folder_total_boxes}\")\n",
    "\n",
    "    # 2. 平均分和方差 (需要判断列表是否为空)\n",
    "    if len(all_scores_in_folder) > 0:\n",
    "        # 使用 numpy 计算平均值和方差\n",
    "        folder_mean = np.mean(all_scores_in_folder)\n",
    "        folder_variance = np.var(all_scores_in_folder)\n",
    "        \n",
    "        print(f\"Average Confidence:   {folder_mean:.4f}\")\n",
    "        print(f\"Confidence Variance:  {folder_variance:.4f}\")\n",
    "    else:\n",
    "        print(\"Average Confidence:   N/A (No detections)\")\n",
    "        print(\"Confidence Variance:  N/A\")\n",
    "    \n",
    "    # (可选) 打印每张图片的详情\n",
    "    # for stat in per_image_stats:\n",
    "    #     print(f\"  File: {stat['filename']}, Count: {stat['box_count']}, Scores: {stat['scores']}\")\n",
    "\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # --- 1. 配置区域 (Configuration) ---\n",
    "    # 将所有变量定义在这里，不使用全局变量\n",
    "    cfg = {\n",
    "        \"config_path\": \"groundingdino/config/GroundingDINO_SwinT_OGC.py\",\n",
    "        \"weights_path\": \"/home/pearson/Projects/ECE253/GroundingDINO/weights/groundingdino_swint_ogc.pth\",\n",
    "        \"text_prompt\": \"person . car .\",\n",
    "        \"box_threshold\": 0.35,\n",
    "        \"text_threshold\": 0.25,\n",
    "        \"device\": \"cpu\"\n",
    "    }\n",
    "\n",
    "    # 定义要处理的文件夹列表 [(Input, Output), (Input, Output)]\n",
    "    folders_to_process = [\n",
    "        (\n",
    "            \"../dataset\", \n",
    "            \"../results/annoted/original/\"\n",
    "        ),\n",
    "        (\n",
    "            \"../results/CLAHE/\", \n",
    "            \"../results/annoted/CLAHE/\"\n",
    "        ),\n",
    "        (\n",
    "            \"../results/CLAHE_Wiener/\", \n",
    "            \"../results/annoted/CLAHE_Wiener/\"\n",
    "        ),\n",
    "        (\n",
    "            \"../results/CLAHE_NAFNet/\", \n",
    "            \"../results/annoted/CLAHE_NAFNet/\"\n",
    "        ),\n",
    "        (\n",
    "            \"../results/ZeroDCE/\", \n",
    "            \"../results/annoted/ZeroDCE/\"\n",
    "        ),\n",
    "        (\n",
    "            \"../results/ZeroDCE_Wiener/\", \n",
    "            \"../results/annoted/ZeroDCE_Wiener/\"\n",
    "        ),\n",
    "        (\n",
    "            \"../results/ZeroDCE_NAFNet/\", \n",
    "            \"../results/annoted/ZeroDCE_NAFNet/\"\n",
    "        ),\n",
    "        # 在这里添加第二个文件夹...\n",
    "        # (\"/path/to/input2\", \"/path/to/output2\"),\n",
    "    ]\n",
    "\n",
    "    # --- 2. 加载模型 (只加载一次) ---\n",
    "    print(f\"Loading Model from {cfg['weights_path']} ...\")\n",
    "    model = load_model(cfg[\"config_path\"], cfg[\"weights_path\"])\n",
    "\n",
    "    # --- 3. 执行批处理 ---\n",
    "    for input_dir, output_dir in folders_to_process:\n",
    "        process_single_folder(model, input_dir, output_dir, cfg)\n",
    "\n",
    "    print(\"All batch processing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIQE = 4.862559032140067\n"
     ]
    }
   ],
   "source": [
    "import pyiqa\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model = pyiqa.create_metric('niqe')\n",
    "# /home/pearson/Projects/ECE253/Image_process/example_jpg/input/IMG_3583.jpeg\n",
    "img = Image.open('/home/pearson/Projects/ECE253/Image_process/example_jpg/output_CLAHE/output_clahe_IMG_3583.jpeg')\n",
    "\n",
    "score = model(img)\n",
    "print(\"NIQE =\", score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import pyiqa\n",
    "# from transformers import AutoModelForImageQualityAssessment, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "########################################\n",
    "# 1. Sharpness (Tenengrad)\n",
    "########################################\n",
    "def tenengrad(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gx = cv2.Sobel(gray, cv2.CV_64F, 1, 0)\n",
    "    gy = cv2.Sobel(gray, cv2.CV_64F, 0, 1)\n",
    "    return np.mean(gx**2 + gy**2)\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# 3. LOE (Lightness Order Error)\n",
    "########################################\n",
    "def compute_loe(input_img, enhanced_img, resize_dim=100):\n",
    "    \"\"\"\n",
    "    计算 LOE (Lightness Order Error)\n",
    "    :param input_img: 原始低光照图像 (BGR)\n",
    "    :param enhanced_img: 增强后的图像 (BGR)\n",
    "    :param resize_dim: 为了加速，通常会将图像缩放到较小尺寸 (如 100x100)\n",
    "    :return: LOE 值 (越小越好)\n",
    "    \"\"\"\n",
    "    # 1. 转为 V 通道 (HSV) 或 L 通道 (LAB) 来提取亮度，或者直接用最大值\n",
    "    # 论文通常使用 max(R,G,B) 作为亮度层\n",
    "    L_in = np.max(input_img, axis=2).astype(np.float32)\n",
    "    L_out = np.max(enhanced_img, axis=2).astype(np.float32)\n",
    "\n",
    "    # 2. 缩放图像以减少计算量 (这是计算 LOE 的标准操作，否则计算量爆炸)\n",
    "    L_in = cv2.resize(L_in, (resize_dim, resize_dim))\n",
    "    L_out = cv2.resize(L_out, (resize_dim, resize_dim))\n",
    "\n",
    "    # 展平为一维数组\n",
    "    vec_in = L_in.flatten()\n",
    "    vec_out = L_out.flatten()\n",
    "    N = len(vec_in)\n",
    "\n",
    "    # 3. 利用广播机制计算相对顺序矩阵\n",
    "    # creating a matrix of comparisons: matrix[i, j] = vec[i] >= vec[j]\n",
    "    \n",
    "    # 原图的相对顺序矩阵 (如果 i >= j 则为 True)\n",
    "    # 使用广播: vec_in[:, None] 是列向量, vec_in[None, :] 是行向量\n",
    "    order_in = vec_in[:, None] >= vec_in[None, :]\n",
    "    \n",
    "    # 增强图的相对顺序矩阵\n",
    "    order_out = vec_out[:, None] >= vec_out[None, :]\n",
    "\n",
    "    # 4. 计算异或 (XOR)\n",
    "    # 如果原图和增强图的顺序关系不一致 (一个True一个False)，异或结果为 True (1)\n",
    "    difference_matrix = np.logical_xor(order_in, order_out)\n",
    "\n",
    "    # 5. 计算 LOE\n",
    "    # 对所有差异求和，然后归一化\n",
    "    loe_score = np.mean(np.sum(difference_matrix, axis=1))\n",
    "    \n",
    "    return loe_score\n",
    "\n",
    "# 使用示例\n",
    "# input_img = cv2.imread('low.jpg')\n",
    "# enhanced_img = cv2.imread('enhanced.jpg')\n",
    "# score = compute_loe(input_img, enhanced_img)\n",
    "# print(f\"LOE Score: {score}\")\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # 4. Deep metric: MUSIQ\n",
    "# ########################################\n",
    "# def load_musiq():\n",
    "#     model = AutoModelForImageQualityAssessment.from_pretrained(\"google/musiq\")\n",
    "#     processor = AutoImageProcessor.from_pretrained(\"google/musiq\")\n",
    "#     return model, processor\n",
    "\n",
    "\n",
    "# def compute_musiq(img_pil, model, processor):\n",
    "#     inputs = processor(images=img_pil, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         score = model(**inputs).logits.item()\n",
    "#     return score\n",
    "\n",
    "\n",
    "def compute_maniqa(img_pil):\n",
    "    model = pyiqa.create_metric('maniqa')\n",
    "    return model(img_pil).item()\n",
    "\n",
    "\n",
    "########################################\n",
    "# 5. Main pipeline\n",
    "########################################\n",
    "def evaluate(image_path):\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_pil = Image.open(image_path)\n",
    "\n",
    "    # pyiqa metrics\n",
    "    niqe = pyiqa.create_metric('niqe')\n",
    "    brisque = pyiqa.create_metric('brisque')\n",
    "    piqe = pyiqa.create_metric('piqe')\n",
    "\n",
    "    niqe_score = niqe(img_pil).item()\n",
    "    brisque_score = brisque(img_pil).item()\n",
    "    piqe_score = piqe(img_pil).item()\n",
    "\n",
    "    # Classical metrics\n",
    "    tenengrad_score = tenengrad(img)\n",
    "    # entropy_score = entropy(img)\n",
    "    # loe_score = compute_loe(img)\n",
    "\n",
    "    # # MUSIQ\n",
    "    maniqa_score = compute_maniqa(img_pil)\n",
    "    # musiq_model, musiq_processor = load_musiq()\n",
    "    # musiq_score = compute_musiq(img_pil, musiq_model, musiq_processor)\n",
    "\n",
    "    result = {\n",
    "        \"NIQE\": niqe_score,\n",
    "        # \"BRISQUE\": brisque_score,\n",
    "        # \"PIQE\": piqe_score,\n",
    "        \"MANIQ\": maniqa_score,\n",
    "        \"Tenengrad\": tenengrad_score,\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "########################################\n",
    "# Run example\n",
    "########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model MANIQA from /home/pearson/.cache/torch/hub/pyiqa/ckpt_koniq10k.pt\n",
      "{\n",
      "    \"NIQE\": 5.12974739878657,\n",
      "    \"MANIQ\": 0.25075048208236694,\n",
      "    \"Tenengrad\": 823.0470265652557\n",
      "}\n",
      "Loading pretrained model MANIQA from /home/pearson/.cache/torch/hub/pyiqa/ckpt_koniq10k.pt\n",
      "{\n",
      "    \"NIQE\": 3.869808225234333,\n",
      "    \"MANIQ\": 0.20987872779369354,\n",
      "    \"Tenengrad\": 3450.0166497057926\n",
      "}\n",
      "Loading pretrained model MANIQA from /home/pearson/.cache/torch/hub/pyiqa/ckpt_koniq10k.pt\n",
      "{\n",
      "    \"NIQE\": 4.342191332573618,\n",
      "    \"MANIQ\": 0.2231995016336441,\n",
      "    \"Tenengrad\": 4722.289415660169\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--image\", type=str, required=True)\n",
    "# args = parser.parse_args()\n",
    "img1 = '/home/pearson/Projects/ECE253/Image_process/example_jpg/input/IMG_3580.jpeg'\n",
    "scores1 = evaluate(img1)\n",
    "\n",
    "print(json.dumps(scores1, indent=4))\n",
    "\n",
    "\n",
    "img2 = '/home/pearson/Projects/ECE253/Image_process/example_jpg/output_CLAHE/output_clahe_IMG_3580.jpeg'\n",
    "scores2 = evaluate(img2)\n",
    "\n",
    "print(json.dumps(scores2, indent=4))\n",
    "\n",
    "\n",
    "img3 = '/home/pearson/Projects/ECE253/Image_process/example_jpg/output_ZeroDCE/IMG_3580.jpeg'\n",
    "scores3 = evaluate(img3)\n",
    "\n",
    "print(json.dumps(scores3, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816.9036\n",
      "5399.7899\n"
     ]
    }
   ],
   "source": [
    "# input_img = cv2.imread('low.jpg')\n",
    "# enhanced_img = cv2.imread('enhanced.jpg')\n",
    "input_img_path = '/home/pearson/Projects/ECE253/Image_process/example_jpg/input/IMG_3583.jpeg'\n",
    "enhanced_img1_path = '/home/pearson/Projects/ECE253/Image_process/example_jpg/output_CLAHE/output_clahe_IMG_3583.jpeg'\n",
    "enhanced_img2_path = '/home/pearson/Projects/ECE253/Image_process/example_jpg/output_ZeroDCE/IMG_3583.jpeg'\n",
    "\n",
    "input_img = cv2.imread(input_img_path)\n",
    "enhanced_img1 = cv2.imread(enhanced_img1_path)\n",
    "enhanced_img2 = cv2.imread(enhanced_img2_path)\n",
    "score1 = compute_loe(input_img, enhanced_img1)\n",
    "score2 = compute_loe(input_img, enhanced_img2)\n",
    "print(score1)\n",
    "print(score2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometry_splatting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
